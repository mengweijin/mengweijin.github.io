(window.webpackJsonp=window.webpackJsonp||[]).push([[20],{386:function(a,t,e){a.exports=e.p+"assets/img/ConcurrentHashMap.df173b85.png"},475:function(a,t,e){"use strict";e.r(t);var s=e(28),h=Object(s.a)({},(function(){var a=this,t=a.$createElement,s=a._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[s("p",[a._v("ConcurrentHashMap是线程安全且高效的HashMap。")]),a._v(" "),s("h2",{attrs:{id:"hashmap-概述"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#hashmap-概述"}},[a._v("#")]),a._v(" HashMap 概述")]),a._v(" "),s("p",[a._v("HashMap是基于哈希表的Map接口的非同步实现。此实现提供所有可选的映射操作，并允许使用null值和null键（除了不同步和允许使用 null 之外，HashMap 类与 Hashtable 大致相同）。此类不保证映射的顺序，特别是它不保证该顺序恒久不变。")]),a._v(" "),s("p",[a._v("值得注意的是HashMap不是线程安全的，如果想要线程安全的HashMap，可以通过Collections类的静态方法synchronizedMap获得线程安全的HashMap。")]),a._v(" "),s("p",[a._v("Map map = Collections.synchronizedMap(new HashMap());")]),a._v(" "),s("h3",{attrs:{id:"hashmap的数据结构"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#hashmap的数据结构"}},[a._v("#")]),a._v(" HashMap的数据结构")]),a._v(" "),s("p",[a._v("HashMap的底层主要是基于数组和链表来实现的，它之所以有相当快的查询速度主要是因为它是通过计算散列码来决定存储的位置。HashMap中主要是通过key的hashCode来计算hash值的，只要hashCode相同，计算出来的hash值就一样。如果存储的对象对多了，就有可能不同的对象所算出来的hash值是相同的，这就出现了所谓的hash冲突。学过数据结构的同学都知道，解决hash冲突的方法有很多，"),s("strong",[a._v("HashMap底层是通过链表来解决hash冲突的")]),a._v("。也就是说，其链表结果主要是用来解决hash冲突的。")]),a._v(" "),s("p",[a._v("hashmap结构：哈希表是由数组+链表组成的，数组的默认长度为16（可以自动变长。在构造HashMap的时候也可以指定一个长度），数组里每个元素存储的是一个链表的头结点。")]),a._v(" "),s("p",[a._v("JDK1.8中：")]),a._v(" "),s("ul",[s("li",[a._v("使用一个Node数组来存储数据，但这个Node可能是链表结构，也可能是红黑树结构")]),a._v(" "),s("li",[a._v("如果插入的key的hashcode相同，那么这些key也会被定位到Node数组的同一个格子里。")]),a._v(" "),s("li",[a._v("如果同一个格子里的key不超过8个，使用链表结构存储。")]),a._v(" "),s("li",[a._v("如果超过了8个，那么会调用treeifyBin函数，将链表转换为红黑树。")]),a._v(" "),s("li",[a._v("那么即使hashcode完全相同，由于红黑树的特定，查找某个特定元素，也只需要O(log n)的开销")]),a._v(" "),s("li",[a._v("也就是说put/get的操作的时间复杂度只有O(log n)")])]),a._v(" "),s("p",[a._v("备注：当数组大小已经超过64并且链表中的元素个数超过默认设定（8个）时，将链表转化为红黑树")]),a._v(" "),s("h2",{attrs:{id:"为什么要使用-concurrenthashmap"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#为什么要使用-concurrenthashmap"}},[a._v("#")]),a._v(" 为什么要使用 ConcurrentHashMap？")]),a._v(" "),s("p",[a._v("在并发编程中使用HashMap可能导致程序死循环。而使用线程安全的HashTable效率又非常低下，基于以上两个原因，便有了ConcurrentHashMap的登场机会")]),a._v(" "),s("h3",{attrs:{id:"_1-7-中-hashmap-死循环"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-7-中-hashmap-死循环"}},[a._v("#")]),a._v(" 1.7 中 HashMap 死循环")]),a._v(" "),s("p",[a._v("在多线程环境下，使用 HashMap 进行 put 操作会引起死循环，导致 CPU 利 用率接近 100%，HashMap 在并发执行 put 操作时会引起死循环，是因为多线程会导致 HashMap 的 Entry 链表形成环形数据结构，一旦形成环形数据结构，Entry 的 next 节点永远不为空，就会产生死循环获取 Entry。")]),a._v(" "),s("h3",{attrs:{id:"hashmap-一次扩容的过程及发生死循环的原因"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#hashmap-一次扩容的过程及发生死循环的原因"}},[a._v("#")]),a._v(" HashMap 一次扩容的过程及发生死循环的原因")]),a._v(" "),s("p",[a._v("1、取当前 table 的 2 倍作为新 table 的大小\n2、根据算出的新 table 的大小 new 出一个新的 Entry 数组来，名为 newTable\n3、轮询原 table 的每一个位置，将每个位置上连接的 Entry，算出在新 table 上的位置，并以链表形式连接\n4、原 table 上的所有 Entry 全部轮询完毕之后，意味着原 table 上面的所有 Entry 已经移到了新的 table 上，HashMap 中的 table 指向 newTable")]),a._v(" "),s("p",[a._v("总结：")]),a._v(" "),s("p",[a._v("HashMap 之所以在并发下的扩容造成死循环，是因为，多个线程并发进行 时，因为一个线程先期完成了扩容，将原 Map 的链表重新散列到自己的表中，并且链表变成了倒序，后一个线程再扩容时，又进行自己的散列，再次将倒序链表变为正序链表。于是形成了一个环形链表，当 get 表中不存在的元素时，造成死循环。")]),a._v(" "),s("h3",{attrs:{id:"线程不安全的-hashmap"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#线程不安全的-hashmap"}},[a._v("#")]),a._v(" 线程不安全的 HashMap")]),a._v(" "),s("p",[a._v("在多线程环境下，使用HashMap进行put操作会引起死循环，导致CPU利用率接近100%，所以在并发情况下不能使用HashMap。HashMap在并发执行put操作时会引起死循环，是因为多线程会导致HashMap的Entry链表形成环形数据结构，一旦形成环形数据结构，Entry的next节点永远不为空，就会产生死循环获取Entry。")]),a._v(" "),s("h3",{attrs:{id:"效率低下的-hashtable"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#效率低下的-hashtable"}},[a._v("#")]),a._v(" 效率低下的 HashTable")]),a._v(" "),s("p",[a._v("HashTable容器使用synchronized来保证线程安全，但在线程竞争激烈的情况下HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法，其他线程也访问HashTable的同步方法时，会进入阻塞或轮询状态。如线程1使用put进行元素添加，线程2不但不能使用put方法添加元素，也不能使用get方法来获取元素，所以竞争越激烈效率越低。")]),a._v(" "),s("h3",{attrs:{id:"concurrenthashmap-的锁分段技术可有效提升并发访问率"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#concurrenthashmap-的锁分段技术可有效提升并发访问率"}},[a._v("#")]),a._v(" ConcurrentHashMap 的锁分段技术可有效提升并发访问率")]),a._v(" "),s("p",[a._v("HashTable容器在竞争激烈的并发环境下表现出效率低下的原因是所有访问HashTable的线程都必须竞争同一把锁，假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术。首先将数据分成一段一段地存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。")]),a._v(" "),s("h2",{attrs:{id:"concurrenthashmap-的结构"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#concurrenthashmap-的结构"}},[a._v("#")]),a._v(" ConcurrentHashMap 的结构")]),a._v(" "),s("p",[a._v("jdk 1.7 中采用Segment + HashEntry的方式进行实现，java 8 取消了分段锁的设计，采用数组 + 单向链表 + 红黑树。")]),a._v(" "),s("p",[s("img",{attrs:{src:e(386),alt:"image"}})]),a._v(" "),s("h3",{attrs:{id:"jdk-1-7-的实现方式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#jdk-1-7-的实现方式"}},[a._v("#")]),a._v(" JDK 1.7 的实现方式")]),a._v(" "),s("p",[a._v("jdk 1.7 中，ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。Segment是一种可重入锁（ReentrantLock），在ConcurrentHashMap里扮演锁的角色；HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组。Segment的结构和HashMap类似，是一种数组和链表结构。一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得与它对应的Segment锁。")]),a._v(" "),s("p",[a._v("ConcurrentHashMap初始化时，计算出Segment数组的大小 size 和每个 Segment 中 HashEntry 数组的大小cap，并初始化Segment数组的第一个元素；其中size大小为2的幂次方，默认为16，cap大小也是2的幂次方，最小值为2，最终结果根据根据初始化容量initialCapacity进行计算，其中Segment在实现上继承了ReentrantLock，这样就自带了锁的功能。")]),a._v(" "),s("h4",{attrs:{id:"put实现"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#put实现"}},[a._v("#")]),a._v(" put实现")]),a._v(" "),s("p",[a._v("当执行put方法插入数据时，根据key的hash值，在Segment数组中找到相应的位置，如果相应位置的Segment还未初始化，则通过CAS进行赋值，接着执行Segment对象的put方法通过加锁机制插入数据。")]),a._v(" "),s("ol",[s("li",[a._v("线程A执行tryLock()方法成功获取锁，则把HashEntry对象插入到相应的位置；")]),a._v(" "),s("li",[a._v("线程B获取锁失败，则执行scanAndLockForPut()方法，在scanAndLockForPut方法中，会通过重复执行tryLock()方法尝试获取锁，在多处理器环境下，重复次数为64，单处理器重复次数为1，当执行tryLock()方法的次数超过上限时，则执行lock()方法挂起线程B；")]),a._v(" "),s("li",[a._v("当线程A执行完插入操作时，会通过unlock()方法释放锁，接着唤醒线程B继续执行；")])]),a._v(" "),s("h4",{attrs:{id:"size-实现"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#size-实现"}},[a._v("#")]),a._v(" size 实现")]),a._v(" "),s("p",[a._v("因为ConcurrentHashMap是可以并发插入数据的，所以在准确计算元素时存在一定的难度，一般的思路是统计每个Segment对象中的元素个数，然后进行累加，但是这种方式计算出来的结果并不一样的准确的，因为在计算后面几个Segment的元素个数时，已经计算过的Segment同时可能有数据的插入或则删除，在1.7的实现中，采用了如下方式：")]),a._v(" "),s("p",[a._v("先采用不加锁的方式，连续计算元素的个数，最多计算3次：")]),a._v(" "),s("ol",[s("li",[a._v("如果前后两次计算结果相同，则说明计算出来的元素个数是准确的；")]),a._v(" "),s("li",[a._v("如果前后两次计算结果都不同，则给每个 Segment 进行加锁，再计算一次元素的个数；")])]),a._v(" "),s("h3",{attrs:{id:"jdk-1-8-的实现方式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#jdk-1-8-的实现方式"}},[a._v("#")]),a._v(" JDK 1.8 的实现方式")]),a._v(" "),s("p",[a._v("ConcurrentHashMap在1.8中的实现，相比于1.7的版本基本上全部都变掉了。首先，取消了Segment分段锁的数据结构，取而代之的是数组+链表（红黑树）的结构。而对于锁的粒度，调整为对每个数组元素加锁（Node）。然后是定位节点的hash算法被简化了，这样带来的弊端是Hash冲突会加剧。因此在链表节点数量大于8时，会将链表转化为红黑树进行存储。这样一来，查询的时间复杂度就会由原先的O(n)变为O(logN)。")]),a._v(" "),s("p",[a._v("1.8中放弃了Segment臃肿的设计，取而代之的是采用Node + CAS + Synchronized来保证并发安全进行实现，")]),a._v(" "),s("p",[a._v("只有在执行第一次put方法时才会调用initTable()初始化Node数组。")]),a._v(" "),s("h4",{attrs:{id:"put实现-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#put实现-2"}},[a._v("#")]),a._v(" put实现")]),a._v(" "),s("p",[a._v("当执行put方法插入数据时，根据key的hash值，在Node数组中找到相应的位置，实现如下：")]),a._v(" "),s("ol",[s("li",[a._v("如果相应位置的Node还未初始化，则通过CAS插入相应的数据；")]),a._v(" "),s("li",[a._v("如果相应位置的Node不为空，且当前该节点不处于移动状态，则对该节点加synchronized锁，如果该节点的hash不小于0，则遍历链表更新节点或插入新节点；")]),a._v(" "),s("li",[a._v("如果该节点是TreeBin类型的节点，说明是红黑树结构，则通过putTreeVal方法往红黑树中插入节点；")]),a._v(" "),s("li",[a._v("如果baseCount不为0，说明put操作对数据产生了影响，如果当前链表的个数达到8个，则通过treeifyBin方法转化为红黑树，如果oldVal不为空，说明是一次更新操作，没有对元素个数产生影响，则直接返回旧值；")]),a._v(" "),s("li",[a._v("如果插入的是一个新节点，则执行addCount()方法尝试更新元素个数baseCount；")])]),a._v(" "),s("h4",{attrs:{id:"size-实现-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#size-实现-2"}},[a._v("#")]),a._v(" size 实现")]),a._v(" "),s("p",[a._v("1.8 中使用一个volatile类型的变量baseCount记录元素的个数，当插入新数据或则删除数据时，会通过addCount()方法更新baseCount")]),a._v(" "),s("ol",[s("li",[a._v("初始化时counterCells为空，在并发量很高时，如果存在两个线程同时执行CAS修改baseCount值，则失败的线程会继续执行方法体中的逻辑，使用CounterCell记录元素个数的变化；")]),a._v(" "),s("li",[a._v("如果CounterCell数组counterCells为空，调用fullAddCount()方法进行初始化，并插入对应的记录数，通过CAS设置cellsBusy字段，只有设置成功的线程才能初始化CounterCell数组")]),a._v(" "),s("li",[a._v("如果通过CAS设置cellsBusy字段失败的话，则继续尝试通过CAS修改baseCount字段，如果修改baseCount字段成功的话，就退出循环，否则继续循环插入CounterCell对象；所以在1.8中的size实现比1.7简单多，因为元素个数保存baseCount中，部分元素的变化个数保存在CounterCell数组中")]),a._v(" "),s("li",[a._v("通过累加baseCount和CounterCell数组中的数量，即可得到元素的总个数；")])])])}),[],!1,null,null,null);t.default=h.exports}}]);